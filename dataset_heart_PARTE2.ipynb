{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gw2T-MEZ9Pk5"
      },
      "outputs": [],
      "source": [
        "# Cargar librerías\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "import optuna\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Cargar dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Importamos el DataFrame.\n",
        "path = \"/content/drive/MyDrive/datasets/dataset_heart.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Vista general\n",
        "print(\"Primeras filas:\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\nResumen de tipos de datos:\")\n",
        "display(df.info())\n",
        "\n",
        "# --- EDA ---\n",
        "\n",
        "# 1. VALORES NULOS\n",
        "print(\"\\nConteo de valores nulos por columna:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Si hay pocos valores nulos, se pueden eliminar las filas\n",
        "# Si hay más, imputar según el tipo de dato (media, mediana, moda)\n",
        "\n",
        "# Ejemplo general de imputación:\n",
        "# Por tipo de dato\n",
        "for col in df.columns:\n",
        "    if df[col].isnull().sum() > 0:\n",
        "        if df[col].dtype in ['float64', 'int64']:\n",
        "            median = df[col].median()\n",
        "            df[col].fillna(median, inplace=True)\n",
        "        else:\n",
        "            mode = df[col].mode()[0]\n",
        "            df[col].fillna(mode, inplace=True)\n",
        "\n",
        "print(\"\\nValores nulos después de la imputación:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# 2. OUTLIERS\n",
        "# Usamos boxplots para identificar outliers en variables numéricas\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "for col in numeric_cols:\n",
        "    plt.figure(figsize=(6, 3))\n",
        "    sns.boxplot(data=df, x=col, color='skyblue')\n",
        "    plt.title(f'Distribución y outliers: {col}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Eliminación de outliers usando el método IQR\n",
        "\n",
        "def remove_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    filtered_df = df[(df[column] >= lower) & (df[column] <= upper)]\n",
        "    return filtered_df\n",
        "\n",
        "# Aplicamos IQR para columnas con outliers claros (ejemplo: 'chol', 'trestbps')\n",
        "cols_to_filter = ['chol', 'trestbps', 'thalach', 'oldpeak']  # personalizable\n",
        "\n",
        "for col in cols_to_filter:\n",
        "    original_shape = df.shape\n",
        "    df = remove_outliers_iqr(df, col)\n",
        "    new_shape = df.shape\n",
        "    print(f\"Filtrados outliers en '{col}': {original_shape[0] - new_shape[0]} filas eliminadas.\")\n",
        "\n",
        "# --- TRANSFORMACION COLUMNAS ---\n",
        "\n",
        "# Separar variables numéricas y categóricas\n",
        "num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# También incluimos como categóricas algunas variables tipo int que son en realidad cualitativas\n",
        "# como 'cp', 'thal', 'slope', etc., si están en el dataset como int\n",
        "\n",
        "# Supongamos que estas son categóricas codificadas como int:\n",
        "cols_categoricas_extra = ['cp', 'thal', 'slope', 'restecg', 'ca', 'sex', 'fbs', 'exang']\n",
        "for col in cols_categoricas_extra:\n",
        "    if col in df.columns and col not in cat_cols:\n",
        "        cat_cols.append(col)\n",
        "        if col in num_cols:\n",
        "            num_cols.remove(col)\n",
        "\n",
        "# Separar X (features) e y (target)\n",
        "X = df.drop(columns=['target']) if 'target' in df.columns else df.drop(columns=['Target'])\n",
        "y = df['target'] if 'target' in df.columns else df['Target']\n",
        "\n",
        "# Crear el ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Aplicamos la transformación\n",
        "X_transformed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Mostrar resultado\n",
        "print(\"Forma de X antes:\", X.shape)\n",
        "print(\"Forma de X después de la transformación:\", X_transformed.shape)\n",
        "\n",
        "\n",
        "# --- PIPELINE ---\n",
        "\n",
        "# --- Definir columnas numéricas y categóricas  ---\n",
        "num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "cols_categoricas_extra = ['cp', 'thal', 'slope', 'restecg', 'ca', 'sex', 'fbs', 'exang']\n",
        "for col in cols_categoricas_extra:\n",
        "    if col in df.columns and col not in cat_cols:\n",
        "        cat_cols.append(col)\n",
        "        if col in num_cols:\n",
        "            num_cols.remove(col)\n",
        "\n",
        "# Separar X y y\n",
        "X = df.drop(columns=['target']) if 'target' in df.columns else df.drop(columns=['Target'])\n",
        "y = df['target'] if 'target' in df.columns else df['Target']\n",
        "\n",
        "# --- Pipelines para imputación y preprocesamiento ---\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # imputa con mediana\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, num_cols),\n",
        "    ('cat', categorical_transformer, cat_cols)\n",
        "])\n",
        "\n",
        "# Pipeline final con modelo\n",
        "model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# División de datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entrenar pipeline\n",
        "model_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predecir y evaluar\n",
        "y_pred = model_pipeline.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# --- ENTRENAMIENTO INICIAL ---\n",
        "\n",
        "# Lista de modelos a evaluar\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
        "    'LightGBM': lgb.LGBMClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "# Diccionario para guardar resultados\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Evaluando: {name}\")\n",
        "\n",
        "    # Crear pipeline con preprocesador + modelo\n",
        "    pipe = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "\n",
        "    # Validación cruzada con 5 folds, métrica accuracy (puedes usar scoring='f1' u otra)\n",
        "    scores = cross_val_score(pipe, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "    results[name] = scores\n",
        "    print(f\"Accuracy promedio: {scores.mean():.4f} ± {scores.std():.4f}\\n\")\n",
        "\n",
        "# Mostrar resumen de resultados\n",
        "print(\"Resumen de modelos evaluados:\")\n",
        "for name, scores in results.items():\n",
        "    print(f\"{name}: Accuracy media = {scores.mean():.4f}, std = {scores.std():.4f}\")\n",
        "\n",
        "# Mejor modelo\n",
        "best_model_name = max(results, key=lambda k: results[k].mean())\n",
        "print(f\"\\nMejor modelo inicial según accuracy: {best_model_name}\")\n",
        "\n",
        "# --- OPTIMIZACION DE HIPERPARAMETROS ---\n",
        "\n",
        "# Espacio de búsqueda (hiperparámetros relevantes para Random Forest)\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [50, 100, 200],\n",
        "    'classifier__max_depth': [None, 10, 20, 30],\n",
        "    'classifier__min_samples_split': [2, 5, 10],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Crear pipeline con preprocesador y RF base\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# GridSearchCV con 5 folds\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Ejecutar búsqueda\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Mejores hiperparámetros encontrados con GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "print(f\"Mejor score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Espacio de búsqueda más amplio (distribuciones)\n",
        "param_dist = {\n",
        "    'classifier__n_estimators': randint(50, 300),\n",
        "    'classifier__max_depth': [None] + list(range(5, 50, 5)),\n",
        "    'classifier__min_samples_split': randint(2, 20),\n",
        "    'classifier__min_samples_leaf': randint(1, 20),\n",
        "    'classifier__bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    pipeline, param_distributions=param_dist,\n",
        "    n_iter=50, cv=5, scoring='accuracy', n_jobs=-1, verbose=1, random_state=42\n",
        ")\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Mejores hiperparámetros encontrados con RandomizedSearchCV:\")\n",
        "print(random_search.best_params_)\n",
        "print(f\"Mejor score: {random_search.best_score_:.4f}\")\n",
        "\n",
        "def objective(trial):\n",
        "    n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
        "    max_depth = trial.suggest_int('max_depth', 5, 50)\n",
        "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
        "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n",
        "    bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
        "\n",
        "    # Crear modelo con parámetros sugeridos\n",
        "    clf = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        bootstrap=bootstrap,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Pipeline completo\n",
        "    pipe = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', clf)\n",
        "    ])\n",
        "\n",
        "    # Validación cruzada 5 folds\n",
        "    scores = cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "    return scores.mean()\n",
        "\n",
        "# Crear estudio y optimizar\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "print(\"Mejores hiperparámetros encontrados con Optuna:\")\n",
        "print(study.best_params)\n",
        "print(f\"Mejor score: {study.best_value:.4f}\")\n",
        "\n",
        "# Usando mejores parámetros de Optuna, GridSearchCV o RandomizedSearchCV\n",
        "best_params = study.best_params  # o grid_search.best_params_, random_search.best_params_\n",
        "\n",
        "# Crear modelo con mejores hiperparámetros\n",
        "best_rf = RandomForestClassifier(\n",
        "    n_estimators=best_params.get('n_estimators', 100),\n",
        "    max_depth=best_params.get('max_depth', None),\n",
        "    min_samples_split=best_params.get('min_samples_split', 2),\n",
        "    min_samples_leaf=best_params.get('min_samples_leaf', 1),\n",
        "    bootstrap=best_params.get('bootstrap', True),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Pipeline final\n",
        "final_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', best_rf)\n",
        "])\n",
        "\n",
        "# Entrenar con todo el set de entrenamiento\n",
        "final_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predecir en test\n",
        "y_pred = final_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluar\n",
        "print(\"Reporte clasificación en conjunto de prueba con modelo optimizado:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Modelo inicial sin hiperparámetros ajustados\n",
        "initial_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "initial_pipeline.fit(X_train, y_train)\n",
        "y_pred_init = initial_pipeline.predict(X_test)\n",
        "\n",
        "print(\"Reporte clasificación en conjunto de prueba con modelo inicial:\")\n",
        "print(classification_report(y_test, y_pred_init))"
      ]
    }
  ]
}